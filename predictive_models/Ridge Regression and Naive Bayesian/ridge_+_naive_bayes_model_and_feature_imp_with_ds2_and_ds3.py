# -*- coding: utf-8 -*-
"""Ridge + Naive_bayes model and feature_imp with DS2 and DS3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cB34_pe_z1_tU65yXeaRbSxOP995KEan

## Cross validation model
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold,cross_val_predict
from sklearn.linear_model import RidgeClassifier
from sklearn.utils import resample
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler,LabelBinarizer
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay
from sklearn.linear_model import Lasso
from sklearn.utils import compute_class_weight
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import f1_score, balanced_accuracy_score, roc_auc_score, precision_score, recall_score

# import pymc as pm
# import arviz as az

import random
import math

import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.compose import ColumnTransformer
from sklearn import metrics
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler,LabelBinarizer,OneHotEncoder
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay
from sklearn.linear_model import Lasso

# import pymc as pm
# import arviz as az

from collections import Counter
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from google.colab import drive

# feature selection plot for figure3
# this results are taken from (Lasso regression for feature selection.ipynb) G-drive of wenyu
features = [5, 10, 15, 20, 25, 30, 40, 60, 80, 100, 114]
bal_acc = [0.6453738431888503,0.7094935801293277,0.7185717395396289,0.7307892783315812,0.7330191229474168,0.73623494796288,0.7410538475351932,0.7482282882981207,0.7479912287049028,0.7501208586238969,0.7502878430527866]

plt.figure(figsize =(6,5))
plt.plot(features, bal_acc, marker = 'o', markersize = '12' ,linestyle = '-', color = 'b')

plt.xlabel('Number of features', fontsize = '20', fontweight = 'bold' )
plt.ylabel('Mean balanced accuracy', fontsize = '20', fontweight = 'bold' )
plt.xticks( fontsize = '18' )
plt.yticks( fontsize = '18' )
plt.ylim([0.6,0.8])
#plt.title('Ridge regression with', fontsize = '16', fontweight = 'bold' )

plt.tight_layout()
#plt.show()

plt.savefig('important_feature.tiff', dpi=300)

drive.mount('/content/drive')

# Set working directory
path = '/content/drive/My Drive/Tox21/'

# import training and test data files
X = pd.read_csv(path + 'results/X_train_rescale.csv')
y = pd.read_csv(path + 'results/y_train_rescale.csv')
Xtest = pd.read_csv(path + 'results/X_test_rescale.csv')
ytest = pd.read_csv(path + 'results/y_test_rescale.csv')

print('X_train shape',X.shape)
print('y_train shape',y.shape)
print('X_test shape',Xtest.shape)
print('y_test shape',ytest.shape)

assay_train = pd.read_csv(path + 'results/assay_info_train_rescale.csv')
assay_test = pd.read_csv(path + 'results/assay_info_test_rescale.csv')
print('assay_train shape',assay_train.shape)
print('assay_test shape',assay_test.shape)

X = pd.concat([X, assay_train], axis=1)
Xtest = pd.concat([Xtest, assay_test], axis=1)

# saving list for performance metrics
# Training
bal_accs_t = []
auc_scores_t = []
recalls_t = []
f1s_t = []
precisions_t = []

# Validation
bal_accs_v = []
auc_scores_v = []
recalls_v = []
f1s_v = []
precisions_v = []

# Testing
bal_accs_w = []
auc_scores_w = []
f1_scores = []
recall_scores = []
precision_scores = []

# NBayes
# Training
NB_bal_accs_t = []
NB_auc_scores_t = []
NB_recalls_t = []
NB_f1s_t = []
NB_precisions_t = []

# Validation
NB_bal_accs_v = []
NB_auc_scores_v = []
NB_recalls_v = []
NB_f1s_v = []
NB_precisions_v = []

# Testing
NB_bal_accs_w = []
NB_auc_scores_w = []
NB_f1_scores = []
NB_recall_scores = []
NB_precision_scores = []

# feature_top_40 = ['FractionCSP3', 'VSA_EState3', 'SlogP_VSA10', 'PEOE_VSA14', 'fr_Al_COO',
#                   'MolLogP', 'SlogP_VSA3', 'VSA_EState9', 'VSA_EState7', 'BCUT2D_LOGPLOW',
#                   'qed', 'MinAbsEStateIndex', 'SlogP_VSA1', 'VSA_EState10',
#                   'FpDensityMorgan3', 'fr_methoxy', 'MaxAbsPartialCharge', 'BCUT2D_MWHI',
#                   'fr_ester', 'BCUT2D_MRHI', 'fr_NH1', 'fr_C_O_noCOO',
#                   'MinAbsPartialCharge', 'MinPartialCharge', 'VSA_EState5', 'TPSA',
#                   'SlogP_VSA8', 'SlogP_VSA6', 'fr_NH0', 'PEOE_VSA10', 'SlogP_VSA2',
#                   'fr_Al_OH_noTert', 'BalabanJ', 'Chi4v', 'VSA_EState1', 'VSA_EState4',
#                   'SMR_VSA4', 'fr_Ar_N', 'EState_VSA8', 'BCUT2D_CHGLO']

feature_top_40 = ['fr_C_O', 'TPSA', 'SMR_VSA1', 'NumHAcceptors', 'NumAromaticCarbocycles','NumHeteroatoms',
                  'Chi4n', 'VSA_EState10', 'VSA_EState3', 'Kappa1', 'Chi2v', 'SMR_VSA10', 'SlogP_VSA2',
                  'HallKierAlpha', 'SMR_VSA5', 'VSA_EState7', 'FractionCSP3', 'SlogP_VSA10', 'BCUT2D_LOGPLOW',
                  'VSA_EState2', 'FpDensityMorgan2', 'fr_phenol_noOrthoHbond', 'SlogP_VSA1', 'PEOE_VSA1',
                  'fr_Ar_N', 'BCUT2D_CHGLO', 'fr_ether', 'EState_VSA1', 'MinPartialCharge', 'VSA_EState6',
                  'MaxPartialCharge', 'MinAbsPartialCharge', 'VSA_EState9', 'SMR_VSA3', 'FpDensityMorgan3',
                  'SlogP_VSA12', 'fr_Al_OH_noTert', 'BalabanJ', 'MaxAbsPartialCharge', 'EState_VSA10']
for j in range(5):
    scaler = StandardScaler()
    df = X[X.columns.intersection(feature_top_40)]
    X_test = Xtest[Xtest.columns.intersection(feature_top_40)]
    y = np.array(y)
    y_test = np.array(y_test)

    X_train, X_valid, y_train, y_valid = train_test_split(df, y, test_size=0.25, shuffle=True, random_state = 42)


    near_miss = RandomUnderSampler(random_state = j)
    X_train_resampled, y_train_resampled = near_miss.fit_resample(X_train, y_train)
    print(Counter(y_train_resampled))

    X_train = scaler.fit_transform(X_train_resampled)
    X_valid = scaler.transform(X_valid)
    X_test = scaler.fit_transform(X_test)

    logistic_cv_balanced = LogisticRegressionCV(penalty='l2',Cs=10,
                                                  max_iter=10000,class_weight=None).fit(X_train, y_train_resampled)

    clf_NB = BernoulliNB().fit(X_train, y_train_resampled)


    ## model predictions
    y_train_pred = logistic_cv_balanced.predict(X_train)
    y_valid_pred = logistic_cv_balanced.predict(X_valid)
    y_test_pred = logistic_cv_balanced.predict(X_test)

    y_train_pred_prob = logistic_cv_balanced.predict_proba(X_train)[:,1]
    y_valid_pred_prob = logistic_cv_balanced.predict_proba(X_valid)[:,1]
    y_test_pred_prob = logistic_cv_balanced.predict_proba(X_test)[:,1]

    ## model performance metrics
    bal_acc_t = balanced_accuracy_score(y_train_resampled,y_train_pred)
    bal_acc_v = balanced_accuracy_score(y_valid, y_valid_pred)
    bal_acc_w = balanced_accuracy_score(y_test,y_test_pred)

    auc_score_t = metrics.roc_auc_score(y_train_resampled, y_train_pred_prob)
    auc_score_v = metrics.roc_auc_score(y_valid, y_valid_pred_prob)
    auc_score_w = metrics.roc_auc_score(y_test, y_test_pred_prob)

    recall_t = metrics.recall_score(y_train_resampled,y_train_pred)
    recall_v = metrics.recall_score(y_valid, y_valid_pred)
    recall_score = metrics.recall_score(y_test, y_test_pred)

    f1_t = metrics.f1_score(y_train_resampled,y_train_pred)
    f1_v = metrics.f1_score(y_valid, y_valid_pred)
    f1_score = metrics.f1_score(y_test, y_test_pred)

    precision_t = metrics.precision_score(y_train_resampled,y_train_pred)
    precision_v = metrics.precision_score(y_valid, y_valid_pred)
    precision_score = metrics.precision_score(y_test, y_test_pred)

    ## save perfomance
    ## Training
    bal_accs_t.append(bal_acc_t)
    auc_scores_t.append(auc_score_t)
    precisions_t.append(precision_t)
    recalls_t.append(recall_t)
    f1s_t.append(f1_t)

    ## Validation
    bal_accs_v.append(bal_acc_v)
    auc_scores_v.append(auc_score_v)
    precisions_v.append(precision_v)
    recalls_v.append(recall_v)
    f1s_v.append(f1_v)

    ## Testing
    bal_accs_w.append(bal_acc_w)
    auc_scores_w.append(auc_score_w)
    precision_scores.append(precision_score)
    recall_scores.append(recall_score)
    f1_scores.append(f1_score)



# Naive Bayes


 ## model predictions
    NB_y_train_pred = clf_NB.predict(X_train)
    NB_y_valid_pred = clf_NB.predict(X_valid)
    NB_y_test_pred = clf_NB.predict(X_test)

    NB_y_train_pred_prob = clf_NB.predict_proba(X_train)[:,1]
    NB_y_valid_pred_prob = clf_NB.predict_proba(X_valid)[:,1]
    NB_y_test_pred_prob = clf_NB.predict_proba(X_test)[:,1]

    ## model performance metrics
    NB_bal_acc_t = balanced_accuracy_score(y_train_resampled,NB_y_train_pred)
    NB_bal_acc_v = balanced_accuracy_score(y_valid, NB_y_valid_pred)
    NB_bal_acc_w = balanced_accuracy_score(y_test,NB_y_test_pred)

    NB_auc_score_t = metrics.roc_auc_score(y_train_resampled, NB_y_train_pred_prob)
    NB_auc_score_v = metrics.roc_auc_score(y_valid, NB_y_valid_pred_prob)
    NB_auc_score_w = metrics.roc_auc_score(y_test, NB_y_test_pred_prob)

    NB_recall_t = metrics.recall_score(y_train_resampled,NB_y_train_pred)
    NB_recall_v = metrics.recall_score(y_valid, NB_y_valid_pred)
    NB_recall_score = metrics.recall_score(y_test, NB_y_test_pred)

    NB_f1_t = metrics.f1_score(y_train_resampled,NB_y_train_pred)
    NB_f1_v = metrics.f1_score(y_valid, NB_y_valid_pred)
    NB_f1_score = metrics.f1_score(y_test, NB_y_test_pred)

    NB_precision_t = metrics.precision_score(y_train_resampled,NB_y_train_pred)
    NB_precision_v = metrics.precision_score(y_valid, NB_y_valid_pred)
    NB_precision_score = metrics.precision_score(y_test, NB_y_test_pred)

    ## save perfomance
    ## Training
    NB_bal_accs_t.append(NB_bal_acc_t)
    NB_auc_scores_t.append(NB_auc_score_t)
    NB_precisions_t.append(NB_precision_t)
    NB_recalls_t.append(NB_recall_t)
    NB_f1s_t.append(NB_f1_t)

    ## Validation
    NB_bal_accs_v.append(NB_bal_acc_v)
    NB_auc_scores_v.append(NB_auc_score_v)
    NB_precisions_v.append(NB_precision_v)
    NB_recalls_v.append(NB_recall_v)
    NB_f1s_v.append(NB_f1_v)

    ## Testing
    NB_bal_accs_w.append(NB_bal_acc_w)
    NB_auc_scores_w.append(NB_auc_score_w)
    NB_precision_scores.append(NB_precision_score)
    NB_recall_scores.append(NB_recall_score)
    NB_f1_scores.append(NB_f1_score)
    print(len(NB_auc_scores_t))
    print(j)

"""## NEW with NEW 40 features"""

## Training set
print('LR_BACC=', np.mean(bal_accs_t))
print(np.std(bal_accs_t))

print('LR_auc=', np.mean(auc_scores_t))
print(np.std(auc_scores_t))

print('LR_f1=', np.mean(f1s_t))
print(np.std(f1s_t))

print('LR_recall=', np.mean(recalls_t))
print(np.std(recalls_t))

print('LR_precision=', np.mean(precisions_t))
print(np.std(precisions_t))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_t))
print(np.std(NB_bal_accs_t))

print('NB_auc=', np.mean(NB_auc_scores_t))
print(np.std(NB_auc_scores_t))

print('NB_f1=', np.mean(NB_f1s_t))
print(np.std(NB_f1s_t))

print('NB_recall=', np.mean(NB_recalls_t))
print(np.std(NB_recalls_t))

print('NB_precision=', np.mean(NB_precisions_t))
print(np.std(NB_precisions_t))

####
# Results of validation data set undersampling 75/25 split training


print('LR_BACC=', np.mean(bal_accs_v))
print(np.std(bal_accs_v))

print('LR_auc=', np.mean(auc_scores_v))
print(np.std(auc_scores_v))

print('LR_f1=', np.mean(f1s_v))
print(np.std(f1s_v))

print('LR_recall=', np.mean(recalls_v))
print(np.std(recalls_v))

print('LR_precision=', np.mean(precisions_v))
print(np.std(precisions_v))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_v))
print(np.std(NB_bal_accs_v))

print('NB_auc=', np.mean(NB_auc_scores_v))
print(np.std(NB_auc_scores_v))

print('NB_f1=', np.mean(NB_f1s_v))
print(np.std(NB_f1s_v))

print('NB_recall=', np.mean(NB_recalls_v))
print(np.std(NB_recalls_v))

print('NB_precision=', np.mean(NB_precisions_v))
print(np.std(NB_precisions_v))

#Results of test data 20% split training

print('LR_BACC=', np.mean(bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(bal_accs_w))

print('LR_auc=', np.mean(auc_scores_w))
print(np.std(auc_scores_w))

print('LR_f1=', np.mean(f1_scores))
print(np.std(f1_scores))

print('LR_recall=', np.mean(recall_scores))
print(np.std(recall_scores))

print('LR_precision=', np.mean(precision_scores))
print(np.std(precision_scores))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(NB_bal_accs_w))

print('NB_auc=', np.mean(NB_auc_scores_w))
print(np.std(NB_auc_scores_w))

print('NB_f1=', np.mean(NB_f1_scores))
print(np.std(NB_f1_scores))

print('NB_recall=', np.mean(NB_recall_scores))
print(np.std(NB_recall_scores))

print('NB_precision=', np.mean(NB_precision_scores))
print(np.std(NB_precision_scores))

"""## Original with OLD 40 features"""

# Results of training data undersampling 75/25 split training

print('LR_BACC=', np.mean(bal_accs_t))
print(np.std(bal_accs_t))

print('LR_auc=', np.mean(auc_scores_t))
print(np.std(auc_scores_t))

print('LR_f1=', np.mean(f1s_t))
print(np.std(f1s_t))

print('LR_recall=', np.mean(recalls_t))
print(np.std(recalls_t))

print('LR_precision=', np.mean(precisions_t))
print(np.std(precisions_t))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_t))
print(np.std(NB_bal_accs_t))

print('NB_auc=', np.mean(NB_auc_scores_t))
print(np.std(NB_auc_scores_t))

print('NB_f1=', np.mean(NB_f1s_t))
print(np.std(NB_f1s_t))

print('NB_recall=', np.mean(NB_recalls_t))
print(np.std(NB_recalls_t))

print('NB_precision=', np.mean(NB_precisions_t))
print(np.std(NB_precisions_t))

# Results of validation data set undersampling 75/25 split training


print('LR_BACC=', np.mean(bal_accs_v))
print(np.std(bal_accs_v))

print('LR_auc=', np.mean(auc_scores_v))
print(np.std(auc_scores_v))

print('LR_f1=', np.mean(f1s_v))
print(np.std(f1s_v))

print('LR_recall=', np.mean(recalls_v))
print(np.std(recalls_v))

print('LR_precision=', np.mean(precisions_v))
print(np.std(precisions_v))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_v))
print(np.std(NB_bal_accs_v))

print('NB_auc=', np.mean(NB_auc_scores_v))
print(np.std(NB_auc_scores_v))

print('NB_f1=', np.mean(NB_f1s_v))
print(np.std(NB_f1s_v))

print('NB_recall=', np.mean(NB_recalls_v))
print(np.std(NB_recalls_v))

print('NB_precision=', np.mean(NB_precisions_v))
print(np.std(NB_precisions_v))

#Results of test data 20% split training

print('LR_BACC=', np.mean(bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(bal_accs_w))

print('LR_auc=', np.mean(auc_scores_w))
print(np.std(auc_scores_w))

print('LR_f1=', np.mean(f1_scores))
print(np.std(f1_scores))

print('LR_recall=', np.mean(recall_scores))
print(np.std(recall_scores))

print('LR_precision=', np.mean(precision_scores))
print(np.std(precision_scores))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(NB_bal_accs_w))

print('NB_auc=', np.mean(NB_auc_scores_w))
print(np.std(NB_auc_scores_w))

print('NB_f1=', np.mean(NB_f1_scores))
print(np.std(NB_f1_scores))

print('NB_recall=', np.mean(NB_recall_scores))
print(np.std(NB_recall_scores))

print('NB_precision=', np.mean(NB_precision_scores))
print(np.std(NB_precision_scores))

"""## Model with Organism and gender"""

# import training and test data files
X = pd.read_csv(path + 'results/X_train_rescale.csv')
y = pd.read_csv(path + 'results/y_train_rescale.csv')
Xtest = pd.read_csv(path + 'results/X_test_rescale.csv')
ytest = pd.read_csv(path + 'results/y_test_rescale.csv')

print('X_train shape',X.shape)
print('y_train shape',y.shape)
print('X_test shape',Xtest.shape)
print('y_test shape',ytest.shape)

df = pd.read_csv(path + 'final_data/df_stack_50.csv')

X = df.drop(columns = ['Outcome', 'Unnamed: 0'])
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=X['ProtocolName'], shuffle=True)

X = X_train.iloc[:,:-7].reset_index(drop=True)
Xtest = X_test.iloc[:,:-7].reset_index(drop=True)

# One-hot encoded gender and organism
label_binarizer = LabelBinarizer()
Organims = pd.DataFrame(label_binarizer.fit_transform(X_train['Organism']), columns=['Organism'])
Organims_test = pd.DataFrame(label_binarizer.transform(X_test['Organism']), columns=['Organism'])
label_binarizer = LabelBinarizer()
Gender = pd.DataFrame(label_binarizer.fit_transform(X_train['Gender']), columns=['Gender'])
Gender_test = pd.DataFrame(label_binarizer.transform(X_test['Gender']), columns=['Gender'])

X = pd.concat([X, Organims, Gender], axis=1)
Xtest = pd.concat([Xtest, Organims_test, Gender_test], axis=1)
y = y_train.copy(deep=True)

print('X_train shape',X.shape)
print('y_train shape',y.shape)
print('X_test shape',Xtest.shape)
print('y_test shape',y_test.shape)

# saving list for performance metrics
# Training
bal_accs_t = []
auc_scores_t = []
recalls_t = []
f1s_t = []
precisions_t = []

# Validation
bal_accs_v = []
auc_scores_v = []
recalls_v = []
f1s_v = []
precisions_v = []

# Testing
bal_accs_w = []
auc_scores_w = []
f1_scores = []
recall_scores = []
precision_scores = []

# NBayes
# Training
NB_bal_accs_t = []
NB_auc_scores_t = []
NB_recalls_t = []
NB_f1s_t = []
NB_precisions_t = []

# Validation
NB_bal_accs_v = []
NB_auc_scores_v = []
NB_recalls_v = []
NB_f1s_v = []
NB_precisions_v = []

# Testing
NB_bal_accs_w = []
NB_auc_scores_w = []
NB_f1_scores = []
NB_recall_scores = []
NB_precision_scores = []

feature_top_40 = ['FractionCSP3', 'VSA_EState3', 'SlogP_VSA10', 'PEOE_VSA14', 'fr_Al_COO',
                  'MolLogP', 'SlogP_VSA3', 'VSA_EState9', 'VSA_EState7', 'BCUT2D_LOGPLOW',
                  'qed', 'MinAbsEStateIndex', 'SlogP_VSA1', 'VSA_EState10',
                  'FpDensityMorgan3', 'fr_methoxy', 'MaxAbsPartialCharge', 'BCUT2D_MWHI',
                  'fr_ester', 'BCUT2D_MRHI', 'fr_NH1', 'fr_C_O_noCOO',
                  'MinAbsPartialCharge', 'MinPartialCharge', 'VSA_EState5', 'TPSA',
                  'SlogP_VSA8', 'SlogP_VSA6', 'fr_NH0', 'PEOE_VSA10', 'SlogP_VSA2',
                  'fr_Al_OH_noTert', 'BalabanJ', 'Chi4v', 'VSA_EState1', 'VSA_EState4',
                  'SMR_VSA4', 'fr_Ar_N', 'EState_VSA8', 'BCUT2D_CHGLO', 'Gender', 'Organism']

feature_top_40 = ['fr_C_O', 'TPSA', 'SMR_VSA1', 'NumHAcceptors', 'NumAromaticCarbocycles','NumHeteroatoms',
                  'Chi4n', 'VSA_EState10', 'VSA_EState3', 'Kappa1', 'Chi2v', 'SMR_VSA10', 'SlogP_VSA2',
                  'HallKierAlpha', 'SMR_VSA5', 'VSA_EState7', 'FractionCSP3', 'SlogP_VSA10', 'BCUT2D_LOGPLOW',
                  'VSA_EState2', 'FpDensityMorgan2', 'fr_phenol_noOrthoHbond', 'SlogP_VSA1', 'PEOE_VSA1',
                  'fr_Ar_N', 'BCUT2D_CHGLO', 'fr_ether', 'EState_VSA1', 'MinPartialCharge', 'VSA_EState6',
                  'MaxPartialCharge', 'MinAbsPartialCharge', 'VSA_EState9', 'SMR_VSA3', 'FpDensityMorgan3',
                  'SlogP_VSA12', 'fr_Al_OH_noTert', 'BalabanJ', 'MaxAbsPartialCharge', 'EState_VSA10',
                  'Gender', 'Organism']
for j in range(5):
    scaler = StandardScaler()

    df = X[X.columns.intersection(feature_top_40)]
    X_test = Xtest[Xtest.columns.intersection(feature_top_40)]
    y = np.array(y)
    y_test = np.array(ytest)

    X_train, X_valid, y_train, y_valid = train_test_split(df, y, test_size=0.25, shuffle=True, random_state = 42)

    # undersampling
    near_miss = RandomUnderSampler(random_state = j)
    X_train_resampled, y_train_resampled = near_miss.fit_resample(X_train, y_train)
    print(Counter(y_train_resampled))

    # normalization
    X_train = scaler.fit_transform(X_train_resampled)
    X_valid = scaler.transform(X_valid)
    X_test = scaler.fit_transform(X_test)

    logistic_cv_balanced = LogisticRegressionCV(penalty='l2',Cs=10,
                                                  max_iter=10000,class_weight=None).fit(X_train, y_train_resampled)

    clf_NB = BernoulliNB(alpha = 50).fit(X_train, y_train_resampled)


    ## model predictions
    y_train_pred = logistic_cv_balanced.predict(X_train)
    y_valid_pred = logistic_cv_balanced.predict(X_valid)
    y_test_pred = logistic_cv_balanced.predict(X_test)

    y_train_pred_prob = logistic_cv_balanced.predict_proba(X_train)[:,1]
    y_valid_pred_prob = logistic_cv_balanced.predict_proba(X_valid)[:,1]
    y_test_pred_prob = logistic_cv_balanced.predict_proba(X_test)[:,1]

    ## model performance metrics
    bal_acc_t = balanced_accuracy_score(y_train_resampled,y_train_pred)
    bal_acc_v = balanced_accuracy_score(y_valid, y_valid_pred)
    bal_acc_w = balanced_accuracy_score(y_test,y_test_pred)

    auc_score_t = metrics.roc_auc_score(y_train_resampled, y_train_pred_prob)
    auc_score_v = metrics.roc_auc_score(y_valid, y_valid_pred_prob)
    auc_score_w = metrics.roc_auc_score(y_test, y_test_pred_prob)

    recall_t = metrics.recall_score(y_train_resampled,y_train_pred)
    recall_v = metrics.recall_score(y_valid, y_valid_pred)
    recall_score = metrics.recall_score(y_test, y_test_pred)

    f1_t = metrics.f1_score(y_train_resampled,y_train_pred)
    f1_v = metrics.f1_score(y_valid, y_valid_pred)
    f1_score = metrics.f1_score(y_test, y_test_pred)

    precision_t = metrics.precision_score(y_train_resampled,y_train_pred)
    precision_v = metrics.precision_score(y_valid, y_valid_pred)
    precision_score = metrics.precision_score(y_test, y_test_pred)

    ## save perfomance
    ## Training
    bal_accs_t.append(bal_acc_t)
    auc_scores_t.append(auc_score_t)
    precisions_t.append(precision_t)
    recalls_t.append(recall_t)
    f1s_t.append(f1_t)

    ## Validation
    bal_accs_v.append(bal_acc_v)
    auc_scores_v.append(auc_score_v)
    precisions_v.append(precision_v)
    recalls_v.append(recall_v)
    f1s_v.append(f1_v)

    ## Testing
    bal_accs_w.append(bal_acc_w)
    auc_scores_w.append(auc_score_w)
    precision_scores.append(precision_score)
    recall_scores.append(recall_score)
    f1_scores.append(f1_score)



# Naive Bayes


 ## model predictions
    NB_y_train_pred = clf_NB.predict(X_train)
    NB_y_valid_pred = clf_NB.predict(X_valid)
    NB_y_test_pred = clf_NB.predict(X_test)

    NB_y_train_pred_prob = clf_NB.predict_proba(X_train)[:,1]
    NB_y_valid_pred_prob = clf_NB.predict_proba(X_valid)[:,1]
    NB_y_test_pred_prob = clf_NB.predict_proba(X_test)[:,1]

    ## model performance metrics
    NB_bal_acc_t = balanced_accuracy_score(y_train_resampled,NB_y_train_pred)
    NB_bal_acc_v = balanced_accuracy_score(y_valid, NB_y_valid_pred)
    NB_bal_acc_w = balanced_accuracy_score(y_test,NB_y_test_pred)

    NB_auc_score_t = metrics.roc_auc_score(y_train_resampled, NB_y_train_pred_prob)
    NB_auc_score_v = metrics.roc_auc_score(y_valid, NB_y_valid_pred_prob)
    NB_auc_score_w = metrics.roc_auc_score(y_test, NB_y_test_pred_prob)

    NB_recall_t = metrics.recall_score(y_train_resampled,NB_y_train_pred)
    NB_recall_v = metrics.recall_score(y_valid, NB_y_valid_pred)
    NB_recall_score = metrics.recall_score(y_test, NB_y_test_pred)

    NB_f1_t = metrics.f1_score(y_train_resampled,NB_y_train_pred)
    NB_f1_v = metrics.f1_score(y_valid, NB_y_valid_pred)
    NB_f1_score = metrics.f1_score(y_test, NB_y_test_pred)

    NB_precision_t = metrics.precision_score(y_train_resampled,NB_y_train_pred)
    NB_precision_v = metrics.precision_score(y_valid, NB_y_valid_pred)
    NB_precision_score = metrics.precision_score(y_test, NB_y_test_pred)

    ## save perfomance
    ## Training
    NB_bal_accs_t.append(NB_bal_acc_t)
    NB_auc_scores_t.append(NB_auc_score_t)
    NB_precisions_t.append(NB_precision_t)
    NB_recalls_t.append(NB_recall_t)
    NB_f1s_t.append(NB_f1_t)

    ## Validation
    NB_bal_accs_v.append(NB_bal_acc_v)
    NB_auc_scores_v.append(NB_auc_score_v)
    NB_precisions_v.append(NB_precision_v)
    NB_recalls_v.append(NB_recall_v)
    NB_f1s_v.append(NB_f1_v)

    ## Testing
    NB_bal_accs_w.append(NB_bal_acc_w)
    NB_auc_scores_w.append(NB_auc_score_w)
    NB_precision_scores.append(NB_precision_score)
    NB_recall_scores.append(NB_recall_score)
    NB_f1_scores.append(NB_f1_score)
    print(len(NB_auc_scores_t))
    print(j)

# saving list for performance metrics
# Training
bal_accs_t = []
auc_scores_t = []
recalls_t = []
f1s_t = []
precisions_t = []

# Validation
bal_accs_v = []
auc_scores_v = []
recalls_v = []
f1s_v = []
precisions_v = []

# Testing
bal_accs_w = []
auc_scores_w = []
f1_scores = []
recall_scores = []
precision_scores = []


# NBayes
# Training
NB_bal_accs_t = []
NB_auc_scores_t = []
NB_recalls_t = []
NB_f1s_t = []
NB_precisions_t = []

# Validation
NB_bal_accs_v = []
NB_auc_scores_v = []
NB_recalls_v = []
NB_f1s_v = []
NB_precisions_v = []

# Testing
NB_bal_accs_w = []
NB_auc_scores_w = []
NB_f1_scores = []
NB_recall_scores = []
NB_precision_scores = []


# Others
lambda_sig_w = []
coefs_w = []
X_tests = []
y_tests = []

# feature_top_40 = ['FractionCSP3', 'VSA_EState3', 'SlogP_VSA10', 'PEOE_VSA14', 'fr_Al_COO',
#                   'MolLogP', 'SlogP_VSA3', 'VSA_EState9', 'VSA_EState7', 'BCUT2D_LOGPLOW',
#                   'qed', 'MinAbsEStateIndex', 'SlogP_VSA1', 'VSA_EState10',
#                   'FpDensityMorgan3', 'fr_methoxy', 'MaxAbsPartialCharge', 'BCUT2D_MWHI',
#                   'fr_ester', 'BCUT2D_MRHI', 'fr_NH1', 'fr_C_O_noCOO',
#                   'MinAbsPartialCharge', 'MinPartialCharge', 'VSA_EState5', 'TPSA',
#                   'SlogP_VSA8', 'SlogP_VSA6', 'fr_NH0', 'PEOE_VSA10', 'SlogP_VSA2',
#                   'fr_Al_OH_noTert', 'BalabanJ', 'Chi4v', 'VSA_EState1', 'VSA_EState4',
#                   'SMR_VSA4', 'fr_Ar_N', 'EState_VSA8', 'BCUT2D_CHGLO', 'Gender', 'Organism']

feature_top_40 = ['fr_C_O', 'TPSA', 'SMR_VSA1', 'NumHAcceptors', 'NumAromaticCarbocycles','NumHeteroatoms',
                  'Chi4n', 'VSA_EState10', 'VSA_EState3', 'Kappa1', 'Chi2v', 'SMR_VSA10', 'SlogP_VSA2',
                  'HallKierAlpha', 'SMR_VSA5', 'VSA_EState7', 'FractionCSP3', 'SlogP_VSA10', 'BCUT2D_LOGPLOW',
                  'VSA_EState2', 'FpDensityMorgan2', 'fr_phenol_noOrthoHbond', 'SlogP_VSA1', 'PEOE_VSA1',
                  'fr_Ar_N', 'BCUT2D_CHGLO', 'fr_ether', 'EState_VSA1', 'MinPartialCharge', 'VSA_EState6',
                  'MaxPartialCharge', 'MinAbsPartialCharge', 'VSA_EState9', 'SMR_VSA3', 'FpDensityMorgan3',
                  'SlogP_VSA12', 'fr_Al_OH_noTert', 'BalabanJ', 'MaxAbsPartialCharge', 'EState_VSA10',
                  'Gender', 'Organism']
for j in range(5):
    scaler = StandardScaler()
    df = X[X.columns.intersection(feature_top_40)]
    X_test = Xtest[Xtest.columns.intersection(feature_top_40)]
    y = np.array(y)
    y_test = np.array(y_test)

    X_train, X_valid, y_train, y_valid = train_test_split(df, y, test_size=0.25, shuffle=True, random_state = 42)

    near_miss = RandomUnderSampler(random_state = j)
    X_train_resampled, y_train_resampled = near_miss.fit_resample(X_train, y_train)
    print(Counter(y_train_resampled))
    print(len(X_train_resampled.columns))
    X_train = scaler.fit_transform(X_train_resampled)
    X_valid = scaler.transform(X_valid)
    X_test = scaler.fit_transform(X_test)

    logistic_cv_balanced = LogisticRegressionCV(penalty='l2',Cs=10,
                                                  max_iter=10000,class_weight=None).fit(X_train, y_train_resampled)

    clf_NB = BernoulliNB(alpha = 50).fit(X_train, y_train_resampled)

    ## Others
    coef_w = logistic_cv_balanced.coef_[0]
    coefs_w.append(coef_w)

    X_tests.append(X_test)
    y_tests.append(y_test)

    ## model predictions
    y_train_pred = logistic_cv_balanced.predict(X_train)
    y_valid_pred = logistic_cv_balanced.predict(X_valid)
    y_test_pred = logistic_cv_balanced.predict(X_test)

    y_train_pred_prob = logistic_cv_balanced.predict_proba(X_train)[:,1]
    y_valid_pred_prob = logistic_cv_balanced.predict_proba(X_valid)[:,1]
    y_test_pred_prob = logistic_cv_balanced.predict_proba(X_test)[:,1]

    ## model performance metrics
    bal_acc_t = balanced_accuracy_score(y_train_resampled,y_train_pred)
    bal_acc_v = balanced_accuracy_score(y_valid, y_valid_pred)
    bal_acc_w = balanced_accuracy_score(y_test,y_test_pred)

    auc_score_t = metrics.roc_auc_score(y_train_resampled, y_train_pred_prob)
    auc_score_v = metrics.roc_auc_score(y_valid, y_valid_pred_prob)
    auc_score_w = metrics.roc_auc_score(y_test, y_test_pred_prob)

    recall_t = metrics.recall_score(y_train_resampled,y_train_pred)
    recall_v = metrics.recall_score(y_valid, y_valid_pred)
    recall_score = metrics.recall_score(y_test, y_test_pred)

    f1_t = metrics.f1_score(y_train_resampled,y_train_pred)
    f1_v = metrics.f1_score(y_valid, y_valid_pred)
    f1_score = metrics.f1_score(y_test, y_test_pred)

    precision_t = metrics.precision_score(y_train_resampled,y_train_pred)
    precision_v = metrics.precision_score(y_valid, y_valid_pred)
    precision_score = metrics.precision_score(y_test, y_test_pred)

    ## save perfomance
    ## Training
    bal_accs_t.append(bal_acc_t)
    auc_scores_t.append(auc_score_t)
    precisions_t.append(precision_t)
    recalls_t.append(recall_t)
    f1s_t.append(f1_t)

    ## Validation
    bal_accs_v.append(bal_acc_v)
    auc_scores_v.append(auc_score_v)
    precisions_v.append(precision_v)
    recalls_v.append(recall_v)
    f1s_v.append(f1_v)

    ## Testing
    bal_accs_w.append(bal_acc_w)
    auc_scores_w.append(auc_score_w)
    precision_scores.append(precision_score)
    recall_scores.append(recall_score)
    f1_scores.append(f1_score)




# Naive Bayes


 ## model predictions
    NB_y_train_pred = clf_NB.predict(X_train)
    NB_y_valid_pred = clf_NB.predict(X_valid)
    NB_y_test_pred = clf_NB.predict(X_test)

    NB_y_train_pred_prob = clf_NB.predict_proba(X_train)[:,1]
    NB_y_valid_pred_prob = clf_NB.predict_proba(X_valid)[:,1]
    NB_y_test_pred_prob = clf_NB.predict_proba(X_test)[:,1]

    ## model performance metrics
    NB_bal_acc_t = balanced_accuracy_score(y_train_resampled,NB_y_train_pred)
    NB_bal_acc_v = balanced_accuracy_score(y_valid, NB_y_valid_pred)
    NB_bal_acc_w = balanced_accuracy_score(y_test,NB_y_test_pred)

    NB_auc_score_t = metrics.roc_auc_score(y_train_resampled, NB_y_train_pred_prob)
    NB_auc_score_v = metrics.roc_auc_score(y_valid, NB_y_valid_pred_prob)
    NB_auc_score_w = metrics.roc_auc_score(y_test, NB_y_test_pred_prob)

    NB_recall_t = metrics.recall_score(y_train_resampled,NB_y_train_pred)
    NB_recall_v = metrics.recall_score(y_valid, NB_y_valid_pred)
    NB_recall_score = metrics.recall_score(y_test, NB_y_test_pred)

    NB_f1_t = metrics.f1_score(y_train_resampled,NB_y_train_pred)
    NB_f1_v = metrics.f1_score(y_valid, NB_y_valid_pred)
    NB_f1_score = metrics.f1_score(y_test, NB_y_test_pred)

    NB_precision_t = metrics.precision_score(y_train_resampled,NB_y_train_pred)
    NB_precision_v = metrics.precision_score(y_valid, NB_y_valid_pred)
    NB_precision_score = metrics.precision_score(y_test, NB_y_test_pred)

    ## save perfomance
    ## Training
    NB_bal_accs_t.append(NB_bal_acc_t)
    NB_auc_scores_t.append(NB_auc_score_t)
    NB_precisions_t.append(NB_precision_t)
    NB_recalls_t.append(NB_recall_t)
    NB_f1s_t.append(NB_f1_t)

    ## Validation
    NB_bal_accs_v.append(NB_bal_acc_v)
    NB_auc_scores_v.append(NB_auc_score_v)
    NB_precisions_v.append(NB_precision_v)
    NB_recalls_v.append(NB_recall_v)
    NB_f1s_v.append(NB_f1_v)

    ## Testing
    NB_bal_accs_w.append(NB_bal_acc_w)
    NB_auc_scores_w.append(NB_auc_score_w)
    NB_precision_scores.append(NB_precision_score)
    NB_recall_scores.append(NB_recall_score)
    NB_f1_scores.append(NB_f1_score)

    print(len(auc_scores_t))
    print(j)

"""## Generate horizontal bar plot, using absolute coefficient values"""

coe_list = []
for i in range(5):
  coe_list.append(pd.DataFrame(coefs_w[i].tolist(), columns = [i]))

coef_df = pd.concat(coe_list, axis=1)
coef_df['Features'] = feature_top_40
coef_df = coef_df.set_index('Features')
coef_df

# mean_df_abs = abs(coef_df)
mean_df_abs = coef_df.copy(deep=True)
mean_df_abs['mean_val'] = np.mean(mean_df_abs,axis=1)
mean_df_abs['std_val'] = np.std(mean_df_abs.iloc[:,:-1], axis=1)
mean_df_abs

mean_df_abs.sort_values(by='mean_val')

h_coe = mean_df_abs.sort_values(by=['mean_val'], ascending=True).iloc[:,-2:]
categories = []
for i in h_coe.index:
    if i.startswith('fr_'):
        categories.append('Fraction of a substructure')
    elif i.startswith('PEOE'):
        categories.append('MOE type - Partial Charges')
    elif i.startswith('SlogP'):
        categories.append('MOE type - LogP')
    elif i.startswith('SMR'):
        categories.append('MOE type - MR')
    elif i.startswith('EState') or i.startswith('VSA'):
        categories.append('MOE type - EState')
    elif i.startswith('BCUT2D'):
        categories.append('BCUT type')
    elif i.startswith('Gen'):
        categories.append('assay_meta_info')
    elif i.startswith('Org'):
        categories.append('assay_meta_info')
    else:
        categories.append('physicochemical properties')
h_coe['category'] = categories
h_coe

h_coe.to_csv(path + 'Ridge_feature_imp_DS3.csv')

h_coe_plot = h_coe.reset_index()
f, ax = plt.subplots(figsize=(10, 8))
colors = {'MOE type - EState': '#c8c8c8', 'Fraction of a substructure': '#f0c571', 'MOE type - LogP': '#59a89c',
          'MOE type - MR':'#0b81a2', 'MOE type - Partial Charges': '#e25759',
          'physicochemical properties': '#9d2c00', 'BCUT type': '#7E4794', 'assay_meta_info': 'green'}
plt.barh(data = h_coe_plot, y='Features', width='mean_val', color=[colors[i] for i in h_coe_plot.category])
plt.errorbar(data = h_coe_plot, y='Features', x='mean_val', xerr = 'std_val', fmt ='none', capsize=3)

labels = h_coe['category'].unique()
handles = [plt.Rectangle((0,0),1,1, color=colors[l]) for l in labels]
plt.legend(handles, labels, title="Categories")
plt.title('Ridge Feature importance with DS3')
plt.xlabel('Feature Coefficients')
plt.ylabel('Chemical Descriptors')
plt.savefig(path + 'Feature_importance_Ridge_DS3_abs_val.tiff', dpi=300, bbox_inches='tight')
plt.show()

"""## NEW with NEW 40 features + gender + organism"""

# Results of training data undersampling 75/25 split training

print('LR_BACC=', np.mean(bal_accs_t))
print(np.std(bal_accs_t))

print('LR_auc=', np.mean(auc_scores_t))
print(np.std(auc_scores_t))

print('LR_f1=', np.mean(f1s_t))
print(np.std(f1s_t))

print('LR_recall=', np.mean(recalls_t))
print(np.std(recalls_t))

print('LR_precision=', np.mean(precisions_t))
print(np.std(precisions_t))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_t))
print(np.std(NB_bal_accs_t))

print('NB_auc=', np.mean(NB_auc_scores_t))
print(np.std(NB_auc_scores_t))

print('NB_f1=', np.mean(NB_f1s_t))
print(np.std(NB_f1s_t))

print('NB_recall=', np.mean(NB_recalls_t))
print(np.std(NB_recalls_t))

print('NB_precision=', np.mean(NB_precisions_t))
print(np.std(NB_precisions_t))

# Results of validation data set undersampling 75/25 split validation


print('LR_BACC=', np.mean(bal_accs_v))
print(np.std(bal_accs_v))

print('LR_auc=', np.mean(auc_scores_v))
print(np.std(auc_scores_v))

print('LR_f1=', np.mean(f1s_v))
print(np.std(f1s_v))

print('LR_recall=', np.mean(recalls_v))
print(np.std(recalls_v))

print('LR_precision=', np.mean(precisions_v))
print(np.std(precisions_v))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_v))
print(np.std(NB_bal_accs_v))

print('NB_auc=', np.mean(NB_auc_scores_v))
print(np.std(NB_auc_scores_v))

print('NB_f1=', np.mean(NB_f1s_v))
print(np.std(NB_f1s_v))

print('NB_recall=', np.mean(NB_recalls_v))
print(np.std(NB_recalls_v))

print('NB_precision=', np.mean(NB_precisions_v))
print(np.std(NB_precisions_v))

#Results of test data 20% split training

print('LR_BACC=', np.mean(bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(bal_accs_w))

print('LR_auc=', np.mean(auc_scores_w))
print(np.std(auc_scores_w))

print('LR_f1=', np.mean(f1_scores))
print(np.std(f1_scores))

print('LR_recall=', np.mean(recall_scores))
print(np.std(recall_scores))

print('LR_precision=', np.mean(precision_scores))
print(np.std(precision_scores))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(NB_bal_accs_w))

print('NB_auc=', np.mean(NB_auc_scores_w))
print(np.std(NB_auc_scores_w))

print('NB_f1=', np.mean(NB_f1_scores))
print(np.std(NB_f1_scores))

print('NB_recall=', np.mean(NB_recall_scores))
print(np.std(NB_recall_scores))

print('NB_precision=', np.mean(NB_precision_scores))
print(np.std(NB_precision_scores))

"""## OLD with OLD 40 features + gender + organism"""

# Results of training data undersampling 75/25 split training

print('LR_BACC=', np.mean(bal_accs_t))
print(np.std(bal_accs_t))

print('LR_auc=', np.mean(auc_scores_t))
print(np.std(auc_scores_t))

print('LR_f1=', np.mean(f1s_t))
print(np.std(f1s_t))

print('LR_recall=', np.mean(recalls_t))
print(np.std(recalls_t))

print('LR_precision=', np.mean(precisions_t))
print(np.std(precisions_t))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_t))
print(np.std(NB_bal_accs_t))

print('NB_auc=', np.mean(NB_auc_scores_t))
print(np.std(NB_auc_scores_t))

print('NB_f1=', np.mean(NB_f1s_t))
print(np.std(NB_f1s_t))

print('NB_recall=', np.mean(NB_recalls_t))
print(np.std(NB_recalls_t))

print('NB_precision=', np.mean(NB_precisions_t))
print(np.std(NB_precisions_t))

# Results of validation data set undersampling 75/25 split training


print('LR_BACC=', np.mean(bal_accs_v))
print(np.std(bal_accs_v))

print('LR_auc=', np.mean(auc_scores_v))
print(np.std(auc_scores_v))

print('LR_f1=', np.mean(f1s_v))
print(np.std(f1s_v))

print('LR_recall=', np.mean(recalls_v))
print(np.std(recalls_v))

print('LR_precision=', np.mean(precisions_v))
print(np.std(precisions_v))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_v))
print(np.std(NB_bal_accs_v))

print('NB_auc=', np.mean(NB_auc_scores_v))
print(np.std(NB_auc_scores_v))

print('NB_f1=', np.mean(NB_f1s_v))
print(np.std(NB_f1s_v))

print('NB_recall=', np.mean(NB_recalls_v))
print(np.std(NB_recalls_v))

print('NB_precision=', np.mean(NB_precisions_v))
print(np.std(NB_precisions_v))

#Results of test data 20% split training

print('LR_BACC=', np.mean(bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(bal_accs_w))

print('LR_auc=', np.mean(auc_scores_w))
print(np.std(auc_scores_w))

print('LR_f1=', np.mean(f1_scores))
print(np.std(f1_scores))

print('LR_recall=', np.mean(recall_scores))
print(np.std(recall_scores))

print('LR_precision=', np.mean(precision_scores))
print(np.std(precision_scores))
print('\n')

print('NB_BACC=', np.mean(NB_bal_accs_w))
# print(sem(bal_accs_w))
print(np.std(NB_bal_accs_w))

print('NB_auc=', np.mean(NB_auc_scores_w))
print(np.std(NB_auc_scores_w))

print('NB_f1=', np.mean(NB_f1_scores))
print(np.std(NB_f1_scores))

print('NB_recall=', np.mean(NB_recall_scores))
print(np.std(NB_recall_scores))

print('NB_precision=', np.mean(NB_precision_scores))
print(np.std(NB_precision_scores))

"""## Hierarchical Bayesian Model"""

# import training and test data files
X = pd.read_csv('X_train.csv')
y = pd.read_csv('y_train.csv')
X_test = pd.read_csv('X_test.csv')
y_test = pd.read_csv('y_test.csv')

print('X_train shape',X.shape)
print('y_train shape',y.shape)
print('X_test shape',X_test.shape)
print('y_test shape',y_test.shape)

from sklearn.model_selection import StratifiedKFold
from imblearn.under_sampling import RandomUnderSampler

n_folds = 5
skf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 42)

train_balanced_accuracy = []
test_balanced_accuracy = []



For train_index, test_index in skf.split(X,y):
    X_train, X_test = X[train_index], X[test_index]

X

df = pd.read_csv('feature_importance_50_assay.csv')
df

# Assume the dataframe is stored in a variable called df
# Create an empty dictionary to store the feature votes
feature_votes = {}

# Loop through each assay column
for i in range(1, 51):
  # Sort the dataframe by the feature importance values in descending order
  sorted_df = df.sort_values(by=f"assay{i}", ascending=False)
  # Get the name of the feature with the highest importance value
  top_feature = sorted_df["features"].iloc[0]
  # Increment the feature vote count in the dictionary
  feature_votes[top_feature] = feature_votes.get(top_feature, 0) + 1

# Create a new column in the dataframe called feature_vote
# Map the feature name to the feature vote count from the dictionary
df["feature_vote"] = df["features"].map(feature_votes)

df

# Assume the dataframe is stored in a variable called df
# Create an empty list to store the feature votes
feature_votes = []

# Loop through each assay column
for i in range(1, 51):
  # Sort the dataframe by the feature importance values in descending order
  sorted_df = df.sort_values(by=f"assay{i}", ascending=False)
  # Get the name of the feature with the highest importance value
  top_feature = sorted_df["features"].iloc[0]
  # Append the feature name to the feature votes list
  feature_votes.append(top_feature)

# Create a new column in the dataframe called feature_vote
# Count the frequency of each feature name in the feature votes list
df["feature_vote"] = df["features"].apply(lambda x: feature_votes.count(x))

df

feature_votes

